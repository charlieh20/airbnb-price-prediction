{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import data\n",
    "train = pd.read_csv(\"../data/train.csv\").sort_values('id')\n",
    "train_df = train.loc[:, train.columns != 'price']\n",
    "train_prices = train['price'].values\n",
    "\n",
    "test_df = pd.read_csv(\"../data/test.csv\").sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import preprocessing\n",
    "\n",
    "# Combine data for processing (important for one-hot encoding)\n",
    "combined_df = pd.concat([test_df, train_df], ignore_index=True)\n",
    "\n",
    "# Select which columns to include in the analysis\n",
    "selected_features = list(combined_df.columns)\n",
    "to_remove = ['id', 'scrape_id', 'last_scraped', 'name', 'description', \n",
    "             'picture_url', 'host_id', 'host_name', 'calendar_last_scraped',\n",
    "             'bathrooms_text']\n",
    "for col in to_remove:\n",
    "  selected_features.remove(col)\n",
    "\n",
    "# Process combined data\n",
    "processed_data = preprocessing(combined_df.copy(), selected_features)\n",
    "\n",
    "test_processed = processed_data.iloc[:len(test_df)]\n",
    "train_processed = processed_data.iloc[len(test_df):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limited features testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 18:45:16.298378: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 18:45:37.923716: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483/483 [==============================] - 1s 1ms/step - loss: 1.7438 - accuracy: 0.2532\n",
      "Epoch 2/10\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.7349 - accuracy: 0.2598\n",
      "Epoch 3/10\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.7348 - accuracy: 0.2598\n",
      "Epoch 4/10\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.7335 - accuracy: 0.2598\n",
      "Epoch 5/10\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.7141 - accuracy: 0.2598\n",
      "Epoch 6/10\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.5739 - accuracy: 0.3367\n",
      "Epoch 7/10\n",
      "483/483 [==============================] - 1s 2ms/step - loss: 1.4950 - accuracy: 0.3775\n",
      "Epoch 8/10\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.4753 - accuracy: 0.3805\n",
      "Epoch 9/10\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.4635 - accuracy: 0.3927\n",
      "Epoch 10/10\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.4533 - accuracy: 0.4055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff3650d57c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Select 7 prominent features to train on\n",
    "selected_features = ['latitude', 'longitude', 'accommodates', \n",
    "                     'amenities', 'availability_365', \n",
    "                     'number_of_reviews_ltm', 'host_since']\n",
    "\n",
    "X_train = train_processed[selected_features].copy()\n",
    "y_train = tf.keras.utils.to_categorical(train_prices, num_classes=6)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=7, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(6, activation='softmax'))  # Output layer with softmax activation for classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',  # Use categorical crossentropy for multi-class classification\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "483/483 [==============================] - 1s 2ms/step - loss: 1.7557 - accuracy: 0.2431\n",
      "Epoch 2/15\n",
      "483/483 [==============================] - 1s 2ms/step - loss: 1.7347 - accuracy: 0.2598\n",
      "Epoch 3/15\n",
      "483/483 [==============================] - 1s 2ms/step - loss: 1.7280 - accuracy: 0.2598\n",
      "Epoch 4/15\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.6656 - accuracy: 0.2962\n",
      "Epoch 5/15\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.5571 - accuracy: 0.3684\n",
      "Epoch 6/15\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.5083 - accuracy: 0.3853\n",
      "Epoch 7/15\n",
      "483/483 [==============================] - 1s 2ms/step - loss: 1.4833 - accuracy: 0.3910\n",
      "Epoch 8/15\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.4651 - accuracy: 0.3880\n",
      "Epoch 9/15\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.4532 - accuracy: 0.3945\n",
      "Epoch 10/15\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.4420 - accuracy: 0.4031\n",
      "Epoch 11/15\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.4338 - accuracy: 0.4039\n",
      "Epoch 12/15\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.4293 - accuracy: 0.4106\n",
      "Epoch 13/15\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.4218 - accuracy: 0.4119\n",
      "Epoch 14/15\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.4191 - accuracy: 0.4137\n",
      "Epoch 15/15\n",
      "483/483 [==============================] - 1s 1ms/step - loss: 1.4167 - accuracy: 0.4117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdbe45a8ee0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Select 7 prominent features to train on\n",
    "selected_features = ['latitude', 'longitude', 'accommodates', \n",
    "                     'amenities', 'availability_365', \n",
    "                     'number_of_reviews_ltm', 'host_since', 'host_listings_count',\n",
    "                     'host_is_superhost', 'host_identity_verified', \n",
    "                     'neighbourhood_cleansed', 'room_type', 'has_availability',\n",
    "                     'availability_60', 'calculated_host_listings_count']\n",
    "X_train = train_processed[selected_features].copy()\n",
    "# Convert output data to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(train_prices, num_classes=6)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=15, activation='sigmoid'))\n",
    "model.add(Dense(16, activation='sigmoid'))\n",
    "model.add(Dense(12, activation='sigmoid'))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "model.add(Dense(6, activation='softmax'))  # Output layer with softmax activation for classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',  # Use categorical crossentropy for multi-class classification\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempts to include images and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Images: 100%|██████████| 1000/1000 [02:36<00:00,  6.38image/s]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate, Embedding, Flatten\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_images(ids, max_samples=None):\n",
    "    image_data = []\n",
    "    inclusion_mask = []\n",
    "\n",
    "    for id in tqdm.tqdm(ids[:max_samples], desc=\"Loading Images\", unit=\"image\"):\n",
    "        try:\n",
    "            img = Image.open(f'../data/test_images/{id}.jpg')\n",
    "            img = img.resize((224, 224))  # Resize images to match MobileNetV2 input size\n",
    "            img_array = image.img_to_array(img)\n",
    "            img_array = preprocess_input(img_array)\n",
    "\n",
    "            # Ensure the image array has the correct shape\n",
    "            if img_array.shape == (224, 224, 3):\n",
    "                image_data.append(img_array)\n",
    "                inclusion_mask.append(True)\n",
    "            else:\n",
    "                inclusion_mask.append(False)\n",
    "        except Exception as e:\n",
    "            inclusion_mask.append(False)\n",
    "\n",
    "    return np.array(image_data), inclusion_mask\n",
    "\n",
    "max_samples = 1000\n",
    "image_data, inclusion_mask = load_and_preprocess_images(train_df['id'], max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and pad sequences for descriptions\n",
    "def preprocess_descriptions(descriptions, max_samples=None):\n",
    "    descriptions = [str(desc) for desc in descriptions[:max_samples]]\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(descriptions[:max_samples])\n",
    "    sequences = tokenizer.texts_to_sequences(descriptions[:max_samples])\n",
    "    padded_sequences = pad_sequences(sequences)\n",
    "    return padded_sequences, tokenizer\n",
    "\n",
    "description_data, tokenizer = preprocess_descriptions(train_df['description'][:max_samples][inclusion_mask])\n",
    "price_data = np.array(train_prices[:max_samples][inclusion_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 0s 0us/step\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - 22s 816ms/step - loss: 648.3593 - mae: 10.8530 - accuracy: 0.2497 - val_loss: 6.5833 - val_mae: 1.9903 - val_accuracy: 0.2474\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 17s 729ms/step - loss: 7.7163 - mae: 2.0483 - accuracy: 0.2510 - val_loss: 6.5402 - val_mae: 1.9826 - val_accuracy: 0.2474\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 19s 818ms/step - loss: 6.5868 - mae: 1.9866 - accuracy: 0.2510 - val_loss: 6.5277 - val_mae: 1.9810 - val_accuracy: 0.2474\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 19s 772ms/step - loss: 6.5539 - mae: 1.9818 - accuracy: 0.2510 - val_loss: 6.5134 - val_mae: 1.9792 - val_accuracy: 0.2474\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 18s 757ms/step - loss: 6.5292 - mae: 1.9786 - accuracy: 0.2510 - val_loss: 6.4971 - val_mae: 1.9771 - val_accuracy: 0.2474\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 18s 776ms/step - loss: 6.5077 - mae: 1.9759 - accuracy: 0.2510 - val_loss: 6.4790 - val_mae: 1.9748 - val_accuracy: 0.2474\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 19s 803ms/step - loss: 6.4854 - mae: 1.9732 - accuracy: 0.2510 - val_loss: 6.4595 - val_mae: 1.9723 - val_accuracy: 0.2474\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 19s 779ms/step - loss: 6.4610 - mae: 1.9702 - accuracy: 0.2510 - val_loss: 6.4386 - val_mae: 1.9696 - val_accuracy: 0.2474\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 19s 785ms/step - loss: 6.4372 - mae: 1.9672 - accuracy: 0.2510 - val_loss: 6.4165 - val_mae: 1.9667 - val_accuracy: 0.2474\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 18s 772ms/step - loss: 6.4145 - mae: 1.9642 - accuracy: 0.2510 - val_loss: 6.3933 - val_mae: 1.9637 - val_accuracy: 0.2474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdb6aaa47c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "image_train, image_test, desc_train, desc_test, price_train, price_test = train_test_split(\n",
    "    image_data, description_data, price_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load pre-trained MobileNetV2 model for image processing\n",
    "image_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained image model\n",
    "for layer in image_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the text processing branch\n",
    "desc_input = Input(shape=(description_data.shape[1],))\n",
    "desc_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=description_data.shape[1])(desc_input)\n",
    "desc_flatten = Flatten()(desc_embedding)\n",
    "\n",
    "# Flatten the output of the MobileNetV2 layer\n",
    "image_flatten = Flatten()(image_model.output)\n",
    "\n",
    "# Combine the flattened image and text features\n",
    "combined = concatenate([image_flatten, desc_flatten])\n",
    "\n",
    "# Add additional layers for the final prediction\n",
    "x = Dense(128, activation='relu')(combined)\n",
    "output = Dense(1, activation='linear')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=[image_model.input, desc_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([np.asarray(image_train), np.asarray(desc_train)], np.asarray(price_train), epochs=10, batch_size=32, validation_data=([np.asarray(image_test), np.asarray(desc_test)], np.asarray(price_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
